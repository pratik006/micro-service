NAME: my-release
LAST DEPLOYED: Wed Sep  9 21:43:46 2020
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
configurationOverrides:
  advertised.listeners: EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_BROKER_ID}))
  listener.security.protocol.map: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
external:
  enabled: true
  type: NodePort
helloWorld: Hello World !
kafka:
  additionalPorts: {}
  affinity: {}
  configJob:
    backoffLimit: 6
  configurationOverrides:
    confluent.support.metrics.enable: false
  envOverrides: {}
  external:
    distinct: false
    dns:
      useExternal: true
      useInternal: false
    domain: cluster.local
    enabled: false
    firstListenerPort: 31090
    init:
      image: lwolf/kubectl_deployer
      imagePullPolicy: IfNotPresent
      imageTag: "0.4"
    loadBalancerIP: []
    loadBalancerSourceRanges: []
    servicePort: 19092
    type: NodePort
  global: {}
  headless:
    port: 9092
  image: confluentinc/cp-kafka
  imagePullPolicy: IfNotPresent
  imageTag: 5.0.1
  jmx:
    configMap:
      enabled: true
      overrideConfig: {}
      overrideName: ""
    port: 5555
    whitelistObjectNames:
    - kafka.controller:*
    - kafka.server:*
    - java.lang:*
    - kafka.network:*
    - kafka.log:*
  kafkaHeapOptions: -Xmx1G -Xms1G
  logSubPath: logs
  nodeSelector: {}
  persistence:
    enabled: true
    mountPath: /opt/kafka/data
    size: 1Gi
  podAnnotations: {}
  podDisruptionBudget: {}
  podLabels: {}
  podManagementPolicy: OrderedReady
  prometheus:
    jmx:
      enabled: false
      image: solsson/kafka-prometheus-jmx-exporter@sha256
      imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8
      interval: 10s
      port: 5556
      resources: {}
      scrapeTimeout: 10s
    kafka:
      affinity: {}
      enabled: false
      image: danielqsj/kafka-exporter
      imageTag: v1.2.0
      interval: 10s
      nodeSelector: {}
      port: 9308
      resources: {}
      scrapeTimeout: 10s
      tolerations: []
    operator:
      enabled: false
      prometheusRule:
        enabled: false
        namespace: monitoring
        releaseNamespace: false
        rules:
        - alert: KafkaNoActiveControllers
          annotations:
            message: The number of active controllers in {{ "{{" }} $labels.namespace
              {{ "}}" }} is less than 1. This usually means that some of the Kafka
              nodes aren't communicating properly. If it doesn't resolve itself you
              can try killing the pods (one by one whilst monitoring the under-replicated
              partitions graph).
          expr: max(kafka_controller_kafkacontroller_activecontrollercount_value)
            by (namespace) < 1
          for: 5m
          labels:
            severity: critical
        - alert: KafkaMultipleActiveControllers
          annotations:
            message: The number of active controllers in {{ "{{" }} $labels.namespace
              {{ "}}" }} is greater than 1. This usually means that some of the Kafka
              nodes aren't communicating properly. If it doesn't resolve itself you
              can try killing the pods (one by one whilst monitoring the under-replicated
              partitions graph).
          expr: max(kafka_controller_kafkacontroller_activecontrollercount_value)
            by (namespace) > 1
          for: 5m
          labels:
            severity: critical
        selector:
          prometheus: kube-prometheus
      serviceMonitor:
        namespace: monitoring
        releaseNamespace: false
        selector:
          prometheus: kube-prometheus
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  replicas: 3
  resources: {}
  securityContext: {}
  terminationGracePeriodSeconds: 60
  testsEnabled: true
  tolerations: []
  topics: []
  updateStrategy:
    type: OnDelete
  zookeeper:
    affinity: {}
    command:
    - /bin/bash
    - -xec
    - /config-scripts/run
    enabled: true
    env:
      JMXAUTH: "false"
      JMXDISABLE: "false"
      JMXPORT: 1099
      JMXSSL: "false"
      ZK_HEAP_SIZE: 1G
      ZK_SYNC_LIMIT: 10
      ZK_TICK_TIME: 2000
      ZOO_AUTOPURGE_PURGEINTERVAL: 0
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: 3
      ZOO_INIT_LIMIT: 5
      ZOO_MAX_CLIENT_CNXNS: 60
      ZOO_PORT: 2181
      ZOO_STANDALONE_ENABLED: false
      ZOO_TICK_TIME: 2000
    exporters:
      jmx:
        config:
          lowercaseOutputName: false
          rules:
          - name: zookeeper_$2
            pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+)><>(\w+)
          - labels:
              replicaId: $2
            name: zookeeper_$3
            pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+)><>(\w+)
          - labels:
              memberType: $3
              replicaId: $2
            name: zookeeper_$4
            pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
              name2=(\w+)><>(\w+)
          - labels:
              memberType: $3
              replicaId: $2
            name: zookeeper_$4_$5
            pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
              name2=(\w+), name3=(\w+)><>(\w+)
          startDelaySeconds: 30
        enabled: false
        env: {}
        image:
          pullPolicy: IfNotPresent
          repository: sscaling/jmx-prometheus-exporter
          tag: 0.3.0
        livenessProbe:
          failureThreshold: 8
          httpGet:
            path: /metrics
            port: jmxxp
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 60
        path: /metrics
        ports:
          jmxxp:
            containerPort: 9404
            protocol: TCP
        readinessProbe:
          failureThreshold: 8
          httpGet:
            path: /metrics
            port: jmxxp
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 60
        resources: {}
        serviceMonitor:
          interval: 30s
          scheme: http
          scrapeTimeout: 30s
      zookeeper:
        config:
          logLevel: info
          resetOnScrape: "true"
        enabled: false
        env: {}
        image:
          pullPolicy: IfNotPresent
          repository: josdotso/zookeeper-exporter
          tag: v1.1.2
        livenessProbe:
          failureThreshold: 8
          httpGet:
            path: /metrics
            port: zookeeperxp
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 60
        path: /metrics
        ports:
          zookeeperxp:
            containerPort: 9141
            protocol: TCP
        readinessProbe:
          failureThreshold: 8
          httpGet:
            path: /metrics
            port: zookeeperxp
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 60
        resources: {}
        serviceMonitor:
          interval: 30s
          scheme: http
          scrapeTimeout: 30s
    global: {}
    headless:
      annotations: {}
    image:
      PullPolicy: IfNotPresent
      pullPolicy: IfNotPresent
      repository: zookeeper
      tag: 3.5.5
    jobs:
      chroots:
        activeDeadlineSeconds: 300
        backoffLimit: 5
        completions: 1
        config:
          create: []
        enabled: false
        env: []
        parallelism: 1
        resources: {}
        restartPolicy: Never
    nodeSelector: {}
    persistence:
      accessMode: ReadWriteOnce
      enabled: false
      size: 5Gi
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    port: 2181
    ports:
      client:
        containerPort: 2181
        protocol: TCP
      election:
        containerPort: 3888
        protocol: TCP
      server:
        containerPort: 2888
        protocol: TCP
    priorityClassName: ""
    prometheus:
      serviceMonitor:
        enabled: false
        selector: {}
    replicaCount: 3
    resources: {}
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    service:
      annotations: {}
      ports:
        client:
          port: 2181
          protocol: TCP
          targetPort: client
      type: ClusterIP
    terminationGracePeriodSeconds: 1800
    tolerations: []
    updateStrategy:
      type: RollingUpdate
    url: ""

HOOKS:
---
# Source: order-managememnt/charts/kafka/templates/tests/test_topic_create_consume_produce.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-test-topic-create-consume-produce"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: my-release-test-consume
    image: confluentinc/cp-kafka:5.0.1
    command:
    - sh
    - -c
    - |
      # Create the topic
      kafka-topics --zookeeper my-release-zookeeper:2181 --topic helm-test-topic-create-consume-produce --create --partitions 1 --replication-factor 1 --if-not-exists && \
      # Create a message
      MESSAGE="`date -u`" && \
      # Produce a test message to the topic
      echo "$MESSAGE" | kafka-console-producer --broker-list my-release-kafka:9092 --topic helm-test-topic-create-consume-produce && \
      # Consume a test message from the topic
      kafka-console-consumer --bootstrap-server my-release-kafka-headless:9092 --topic helm-test-topic-create-consume-produce --from-beginning --timeout-ms 2000 --max-messages 1 | grep "$MESSAGE"
  restartPolicy: Never
MANIFEST:
---
# Source: order-managememnt/charts/kafka/charts/zookeeper/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-2.1.0
    release: my-release
    heritage: Helm
    component: server
spec:
  selector:
    matchLabels:
      app: zookeeper
      release: my-release
      component: server
  maxUnavailable: 1
---
# Source: order-managememnt/charts/kafka/charts/zookeeper/templates/config-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-2.1.0
    release: my-release
    heritage: Helm
    component: server
data:
    ok: |
      #!/bin/sh
      echo ruok | nc 127.0.0.1 ${1:-2181}

    ready: |
      #!/bin/sh
      echo ruok | nc 127.0.0.1 ${1:-2181}

    run: |
      #!/bin/bash

      set -a
      ROOT=$(echo /apache-zookeeper-*)

      ZK_USER=${ZK_USER:-"zookeeper"}
      ZK_LOG_LEVEL=${ZK_LOG_LEVEL:-"INFO"}
      ZK_DATA_DIR=${ZK_DATA_DIR:-"/data"}
      ZK_DATA_LOG_DIR=${ZK_DATA_LOG_DIR:-"/data/log"}
      ZK_CONF_DIR=${ZK_CONF_DIR:-"/conf"}
      ZK_CLIENT_PORT=${ZK_CLIENT_PORT:-2181}
      ZK_SERVER_PORT=${ZK_SERVER_PORT:-2888}
      ZK_ELECTION_PORT=${ZK_ELECTION_PORT:-3888}
      ZK_TICK_TIME=${ZK_TICK_TIME:-2000}
      ZK_INIT_LIMIT=${ZK_INIT_LIMIT:-10}
      ZK_SYNC_LIMIT=${ZK_SYNC_LIMIT:-5}
      ZK_HEAP_SIZE=${ZK_HEAP_SIZE:-2G}
      ZK_MAX_CLIENT_CNXNS=${ZK_MAX_CLIENT_CNXNS:-60}
      ZK_MIN_SESSION_TIMEOUT=${ZK_MIN_SESSION_TIMEOUT:- $((ZK_TICK_TIME*2))}
      ZK_MAX_SESSION_TIMEOUT=${ZK_MAX_SESSION_TIMEOUT:- $((ZK_TICK_TIME*20))}
      ZK_SNAP_RETAIN_COUNT=${ZK_SNAP_RETAIN_COUNT:-3}
      ZK_PURGE_INTERVAL=${ZK_PURGE_INTERVAL:-0}
      ID_FILE="$ZK_DATA_DIR/myid"
      ZK_CONFIG_FILE="$ZK_CONF_DIR/zoo.cfg"
      LOG4J_PROPERTIES="$ZK_CONF_DIR/log4j.properties"
      HOST=$(hostname)
      DOMAIN=`hostname -d`
      ZOOCFG=zoo.cfg
      ZOOCFGDIR=$ZK_CONF_DIR
      JVMFLAGS="-Xmx$ZK_HEAP_SIZE -Xms$ZK_HEAP_SIZE"

      APPJAR=$(echo $ROOT/*jar)
      CLASSPATH="${ROOT}/lib/*:${APPJAR}:${ZK_CONF_DIR}:"

      if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
          NAME=${BASH_REMATCH[1]}
          ORD=${BASH_REMATCH[2]}
          MY_ID=$((ORD+1))
      else
          echo "Failed to extract ordinal from hostname $HOST"
          exit 1
      fi

      mkdir -p $ZK_DATA_DIR
      mkdir -p $ZK_DATA_LOG_DIR
      echo $MY_ID >> $ID_FILE

      echo "clientPort=$ZK_CLIENT_PORT" >> $ZK_CONFIG_FILE
      echo "dataDir=$ZK_DATA_DIR" >> $ZK_CONFIG_FILE
      echo "dataLogDir=$ZK_DATA_LOG_DIR" >> $ZK_CONFIG_FILE
      echo "tickTime=$ZK_TICK_TIME" >> $ZK_CONFIG_FILE
      echo "initLimit=$ZK_INIT_LIMIT" >> $ZK_CONFIG_FILE
      echo "syncLimit=$ZK_SYNC_LIMIT" >> $ZK_CONFIG_FILE
      echo "maxClientCnxns=$ZK_MAX_CLIENT_CNXNS" >> $ZK_CONFIG_FILE
      echo "minSessionTimeout=$ZK_MIN_SESSION_TIMEOUT" >> $ZK_CONFIG_FILE
      echo "maxSessionTimeout=$ZK_MAX_SESSION_TIMEOUT" >> $ZK_CONFIG_FILE
      echo "autopurge.snapRetainCount=$ZK_SNAP_RETAIN_COUNT" >> $ZK_CONFIG_FILE
      echo "autopurge.purgeInterval=$ZK_PURGE_INTERVAL" >> $ZK_CONFIG_FILE
      echo "4lw.commands.whitelist=*" >> $ZK_CONFIG_FILE

      for (( i=1; i<=$ZK_REPLICAS; i++ ))
      do
          echo "server.$i=$NAME-$((i-1)).$DOMAIN:$ZK_SERVER_PORT:$ZK_ELECTION_PORT" >> $ZK_CONFIG_FILE
      done

      rm -f $LOG4J_PROPERTIES

      echo "zookeeper.root.logger=$ZK_LOG_LEVEL, CONSOLE" >> $LOG4J_PROPERTIES
      echo "zookeeper.console.threshold=$ZK_LOG_LEVEL" >> $LOG4J_PROPERTIES
      echo "zookeeper.log.threshold=$ZK_LOG_LEVEL" >> $LOG4J_PROPERTIES
      echo "zookeeper.log.dir=$ZK_DATA_LOG_DIR" >> $LOG4J_PROPERTIES
      echo "zookeeper.log.file=zookeeper.log" >> $LOG4J_PROPERTIES
      echo "zookeeper.log.maxfilesize=256MB" >> $LOG4J_PROPERTIES
      echo "zookeeper.log.maxbackupindex=10" >> $LOG4J_PROPERTIES
      echo "zookeeper.tracelog.dir=$ZK_DATA_LOG_DIR" >> $LOG4J_PROPERTIES
      echo "zookeeper.tracelog.file=zookeeper_trace.log" >> $LOG4J_PROPERTIES
      echo "log4j.rootLogger=\${zookeeper.root.logger}" >> $LOG4J_PROPERTIES
      echo "log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender" >> $LOG4J_PROPERTIES
      echo "log4j.appender.CONSOLE.Threshold=\${zookeeper.console.threshold}" >> $LOG4J_PROPERTIES
      echo "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout" >> $LOG4J_PROPERTIES
      echo "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n" >> $LOG4J_PROPERTIES

      if [ -n "$JMXDISABLE" ]
      then
          MAIN=org.apache.zookeeper.server.quorum.QuorumPeerMain
      else
          MAIN="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=$JMXPORT -Dcom.sun.management.jmxremote.authenticate=$JMXAUTH -Dcom.sun.management.jmxremote.ssl=$JMXSSL -Dzookeeper.jmx.log4j.disable=$JMXLOG4J org.apache.zookeeper.server.quorum.QuorumPeerMain"
      fi

      set -x
      exec java -cp "$CLASSPATH" $JVMFLAGS $MAIN $ZK_CONFIG_FILE
---
# Source: order-managememnt/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: order-management-configmap
data:
  myvalue: Hello World !
---
# Source: order-managememnt/templates/pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: kafka-pv-volume-1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
# Source: order-managememnt/templates/pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: kafka-pv-volume-1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
# Source: order-managememnt/templates/pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: kafka-pv-volume-3
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
# Source: order-managememnt/charts/kafka/charts/zookeeper/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper-headless
  labels:
    app: zookeeper
    chart: zookeeper-2.1.0
    release: my-release
    heritage: Helm
spec:
  clusterIP: None
  ports:
    - name: client
      port: 2181
      targetPort: client
      protocol: TCP
    - name: election
      port: 3888
      targetPort: election
      protocol: TCP
    - name: server
      port: 2888
      targetPort: server
      protocol: TCP
  selector:
    app: zookeeper
    release: my-release
---
# Source: order-managememnt/charts/kafka/charts/zookeeper/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-2.1.0
    release: my-release
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: client
      port: 2181
      protocol: TCP
      targetPort: client
  selector:
    app: zookeeper
    release: my-release
---
# Source: order-managememnt/charts/kafka/templates/service-brokers.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka
  labels:
    helm.sh/chart: kafka-0.21.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: my-release
spec:
  ports:
  - name: broker
    port: 9092
    targetPort: kafka
  selector:
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: my-release
---
# Source: order-managememnt/charts/kafka/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka-headless
  labels:
    helm.sh/chart: kafka-0.21.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: my-release
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - name: broker
    port: 9092
  clusterIP: None
  selector:
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: my-release
---
# Source: order-managememnt/templates/kafka-test-client.yaml
apiVersion: v1
kind: Pod
metadata:
  name: testclient
spec:
  containers:
    - name: kafka
      image: solsson/kafka:0.11.0.0
      command:
        - sh
        - -c
        - "exec tail -f /dev/null"
---
# Source: order-managememnt/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-2.1.0
    release: my-release
    heritage: Helm
    component: server
spec:
  serviceName: my-release-zookeeper-headless
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
      release: my-release
      component: server
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: zookeeper
        release: my-release
        component: server
    spec:
      terminationGracePeriodSeconds: 1800
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      containers:

        - name: zookeeper
          image: "zookeeper:3.5.5"
          imagePullPolicy: IfNotPresent
          command: 
             - "/bin/bash"
             - "-xec"
             - "/config-scripts/run"
          ports:
            - name: client
              containerPort: 2181
              protocol: TCP
            - name: election
              containerPort: 3888
              protocol: TCP
            - name: server
              containerPort: 2888
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - sh
                - /config-scripts/ok
            initialDelaySeconds: 20
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 2
            successThreshold: 1
          readinessProbe:
            exec:
              command:
                - sh
                - /config-scripts/ready
            initialDelaySeconds: 20
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 2
            successThreshold: 1
          env:
            - name: ZK_REPLICAS
              value: "3"
            - name: JMXAUTH
              value: "false"
            - name: JMXDISABLE
              value: "false"
            - name: JMXPORT
              value: "1099"
            - name: JMXSSL
              value: "false"
            - name: ZK_HEAP_SIZE
              value: "1G"
            - name: ZK_SYNC_LIMIT
              value: "10"
            - name: ZK_TICK_TIME
              value: "2000"
            - name: ZOO_AUTOPURGE_PURGEINTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_SNAPRETAINCOUNT
              value: "3"
            - name: ZOO_INIT_LIMIT
              value: "5"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_PORT
              value: "2181"
            - name: ZOO_STANDALONE_ENABLED
              value: "false"
            - name: ZOO_TICK_TIME
              value: "2000"
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /data
            - name: config
              mountPath: /config-scripts
      volumes:
        - name: config
          configMap:
            name: my-release-zookeeper
            defaultMode: 0555
        - name: data
          emptyDir: {}
---
# Source: order-managememnt/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-kafka
  labels:
    helm.sh/chart: kafka-0.21.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: my-release
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: kafka-broker
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: my-release
  serviceName: my-release-kafka-headless
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  replicas: 3
  template:
    metadata:
      labels:
        helm.sh/chart: kafka-0.21.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka-broker
        app.kubernetes.io/name: kafka
        app.kubernetes.io/instance: my-release
    spec:
      containers:
      - name: kafka-broker
        image: "confluentinc/cp-kafka:5.0.1"
        imagePullPolicy: "IfNotPresent"
        livenessProbe:
          exec:
            command:
              - sh
              - -ec
              - /usr/bin/jps | /bin/grep -q SupportedKafka
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          tcpSocket:
            port: kafka
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        ports:
        - containerPort: 9092
          name: kafka
        resources:
          {}
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KAFKA_HEAP_OPTS
          value: -Xmx1G -Xms1G
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "my-release-zookeeper:2181"
        - name: KAFKA_LOG_DIRS
          value: "/opt/kafka/data/logs"
        - name: "KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE"
          value: "false"
        - name: KAFKA_JMX_PORT
          value: "5555"
        # This is required because the Downward API does not yet support identification of
        # pod numbering in statefulsets. Thus, we are required to specify a command which
        # allows us to extract the pod ID for usage as the Kafka Broker ID.
        # See: https://github.com/kubernetes/kubernetes/issues/31218
        command:
        - sh
        - -exc
        - |
          unset KAFKA_PORT && \
          export KAFKA_BROKER_ID=${POD_NAME##*-} && \
          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 && \
          exec /etc/confluent/docker/run
        volumeMounts:
        - name: datadir
          mountPath: "/opt/kafka/data"
      volumes:
      terminationGracePeriodSeconds: 60
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

